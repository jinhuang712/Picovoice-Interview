{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picovoice Interview Questions\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given two strings, compute the minimum number of edits\n",
    "needed to transform the first string into the second string. A single edit is an insertion,\n",
    "deletion, or substitution of a single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edits(str1, str2):\n",
    "    if len(str1) == 0:\n",
    "        return len(str2)\n",
    "\n",
    "    if len(str2) == 0:\n",
    "        return len(str1)\n",
    "\n",
    "    if str2[-1] == str1[-1]:\n",
    "        return compute_edits(str1[:-1], str2[:-1])\n",
    "\n",
    "    return 1 + min(compute_edits(str1, str2[:-1]),\n",
    "                   compute_edits(str1[:-1], str2),\n",
    "                   compute_edits(str1[:-1], str2[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_edits(\"Hi\", \"Hil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_edits(\"Take\", \"Jake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_edits(\"fame\", \"famous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that capital letters count as different characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_edits(\"A\", \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an input string and a pattern, implement regular\n",
    "expression matching with support for `.` and `*`.\n",
    "- `.` Matches any single character\n",
    "- `*` Matches zero or more of the preceding element\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_regex(str, pattern):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement (in Numpy) a unidirectional multi-layer LSTM classifier\n",
    "with input and forget gates coupled. You can find information about this variant of\n",
    "LSTM [here](https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer) (look for CIFG). The model should accept a feature vector as input and\n",
    "emit the corresponding posterior. Then train a character-based language model to\n",
    "generate text resembling Shakespear (use any online dataset you see fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You are tasked to collect 1000 hours of “labeled English speech data” for training\n",
    "purposes. The data is pairs of audio files and their corresponding transcripts. The\n",
    "transcripts should be 99%+ accurate. How do you go about this? How fast can you\n",
    "gather 100 hours? How about 10000 hours? Provide as much detail as possible.\n",
    "HINT: TED Talks are freely available for download and are also hand transcripted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By installing this following package,\n",
    "\n",
    "`from youtube_transcript_api import YouTubeTranscriptApi`\n",
    "\n",
    "We could first check if a video has transcripts put-up,\n",
    "\n",
    "`transcript_list = YouTubeTranscriptApi.list_transcripts(video_id, languages=['de', 'en'])`\n",
    "\n",
    "Then automatically downloads the transcripts by running,\n",
    "\n",
    "`transcript = transcript_list.find_transcript(['de', 'en'])`\n",
    "\n",
    "...\n",
    "\n",
    "Next up, we need to download the audio by using the package, \n",
    "\n",
    "`import youtube_dl`\n",
    "\n",
    "With a given download format JSON, which is already specified to downloading audio only (in .mp3 format),\n",
    "\n",
    "```\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "}\n",
    "```\n",
    "\n",
    "We could easily download the audio by,\n",
    "\n",
    "`with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download(['video link'])`\n",
    "    \n",
    "...\n",
    "\n",
    "Finally we just need to extract all the links of the videos of some recognized youtube channels for quality content, clear speech, wide range of topics and often sophisticated words. And for that we would look at education channels, including TED, TEDx Talks, etc. \n",
    "\n",
    "First we need the,\n",
    "\n",
    "`import google-api-python-client`\n",
    "\n",
    "Then, we could get the list of videos by a carefully specified JSON format. \n",
    "\n",
    "Or otherwise we could use the `urllib` api, by doing, for example,\n",
    "\n",
    "`urllib.urlopen(r'http://gdata.youtube.com/feeds/api/videos?start-index={0}&max-results=50&alt=json&orderby=published&author={1}'.format( ind, author ) )`\n",
    "\n",
    "to retrieve a list of youtube urls under a specific channels. \n",
    "\n",
    "Now that we definitely have more than 1000 hours of audio+texts, if that isn't enough, we could reach to audiobooks on youtube which usually are lengthed around 10 hours, including 'To Kill a Mockingbird' (12 hrs+), 'Lord of the Rings' (Book 1 12 hrs+), etc. But it raises another question. These audiobooks don't usually have subtitles with them. Though, there could be a lot of legal issues around this. \n",
    "\n",
    "Another solution is to use the related videos for rescue, and that could be done by,\n",
    "\n",
    "`GET https://www.googleapis.com/youtube/v3/search?part=snippet&relatedToVideoId=____ &type=video&key={YOUR_API_KEY}`\n",
    "\n",
    "Then we could check if that video has subtitles with it. If it does we do what we did above, otherwise we keep going through the related videos list until we have enough hours.\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
